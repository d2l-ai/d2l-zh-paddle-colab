{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed350e1",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 参数管理\n",
    "\n",
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。\n",
    "此时，我们的目标是找到使损失函数最小化的模型参数值。\n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们，\n",
    "将模型保存下来，以便它可以在其他软件中执行，\n",
    "或者为了获得科学的理解而进行检查。\n",
    "\n",
    "之前的介绍中，我们只依靠深度学习框架来完成训练的工作，\n",
    "而忽略了操作参数的具体细节。\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化；\n",
    "* 参数初始化；\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先看一下具有单隐藏层的多层感知机。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22386e07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:50.435818Z",
     "iopub.status.busy": "2023-08-14T21:09:50.435190Z",
     "iopub.status.idle": "2023-08-14T21:09:51.891546Z",
     "shell.execute_reply": "2023-08-14T21:09:51.890605Z"
    },
    "origin_pos": 4,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 1], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
       "       [[0.22011814],\n",
       "        [0.72550499]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import paddle\n",
    "from paddle import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = paddle.rand([2, 4])\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfc3a0",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## [**参数访问**]\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29d11cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.895778Z",
     "iopub.status.busy": "2023-08-14T21:09:51.894964Z",
     "iopub.status.idle": "2023-08-14T21:09:51.900642Z",
     "shell.execute_reply": "2023-08-14T21:09:51.899841Z"
    },
    "origin_pos": 7,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', Parameter containing:\n",
      "Tensor(shape=[8, 1], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
      "       [[-0.60320699],\n",
      "        [ 0.22128904],\n",
      "        [ 0.68972981],\n",
      "        [ 0.54654515],\n",
      "        [-0.52730417],\n",
      "        [ 0.00482953],\n",
      "        [-0.00573695],\n",
      "        [ 0.04480475]])), ('bias', Parameter containing:\n",
      "Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
      "       [0.]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4b215",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "输出的结果告诉我们一些重要的事情：\n",
    "首先，这个全连接层包含两个参数，分别是该层的权重和偏置。\n",
    "两者都存储为单精度浮点数（float32）。\n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。\n",
    "\n",
    "### [**目标参数**]\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f604ea07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.904049Z",
     "iopub.status.busy": "2023-08-14T21:09:51.903506Z",
     "iopub.status.idle": "2023-08-14T21:09:51.908400Z",
     "shell.execute_reply": "2023-08-14T21:09:51.907603Z"
    },
    "origin_pos": 13,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'paddle.fluid.framework.ParamBase'>\n",
      "Parameter containing:\n",
      "Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
      "       [0.])\n",
      "<bound method PyCapsule.value of Parameter containing:\n",
      "Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
      "       [0.])>\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98102386",
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "paddle"
    ]
   },
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。\n",
    "这就是我们需要显式参数值的原因。\n",
    "除了值之外，我们还可以访问每个参数的梯度。\n",
    "在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e979f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.911769Z",
     "iopub.status.busy": "2023-08-14T21:09:51.911120Z",
     "iopub.status.idle": "2023-08-14T21:09:51.916313Z",
     "shell.execute_reply": "2023-08-14T21:09:51.915538Z"
    },
    "origin_pos": 16,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6da8fe",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aaade32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.919663Z",
     "iopub.status.busy": "2023-08-14T21:09:51.919126Z",
     "iopub.status.idle": "2023-08-14T21:09:51.923955Z",
     "shell.execute_reply": "2023-08-14T21:09:51.923161Z"
    },
    "origin_pos": 19,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', [4, 8]) ('bias', [8])\n",
      "('0.weight', [4, 8]) ('0.bias', [8]) ('2.weight', [8, 1]) ('2.bias', [1])\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd38102",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3afe37d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.927395Z",
     "iopub.status.busy": "2023-08-14T21:09:51.926761Z",
     "iopub.status.idle": "2023-08-14T21:09:51.932102Z",
     "shell.execute_reply": "2023-08-14T21:09:51.931283Z"
    },
    "origin_pos": 25,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
       "       [0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231714b",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "### [**从嵌套块收集参数**]\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed153be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.935520Z",
     "iopub.status.busy": "2023-08-14T21:09:51.934884Z",
     "iopub.status.idle": "2023-08-14T21:09:51.949622Z",
     "shell.execute_reply": "2023-08-14T21:09:51.948774Z"
    },
    "origin_pos": 30,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 1], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
       "       [[0.00883046],\n",
       "        [0.03930016]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_sublayer(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7926d72",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "[**设计了网络后，我们看看它是如何工作的。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38da678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.953105Z",
     "iopub.status.busy": "2023-08-14T21:09:51.952410Z",
     "iopub.status.idle": "2023-08-14T21:09:51.956667Z",
     "shell.execute_reply": "2023-08-14T21:09:51.955874Z"
    },
    "origin_pos": 33,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, dtype=float32)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, dtype=float32)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, dtype=float32)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, dtype=float32)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, dtype=float32)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, dtype=float32)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, dtype=float32)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, dtype=float32)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa1679",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。\n",
    "下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dadbd7be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.960148Z",
     "iopub.status.busy": "2023-08-14T21:09:51.959470Z",
     "iopub.status.idle": "2023-08-14T21:09:51.964272Z",
     "shell.execute_reply": "2023-08-14T21:09:51.963405Z"
    },
    "origin_pos": 39,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "Tensor(shape=[8], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(rgnet[0].state_dict()['block 0.0.bias'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5166396",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "## 参数初始化\n",
    "\n",
    "知道了如何访问参数后，现在我们看看如何正确地初始化参数。\n",
    "我们在 :numref:`sec_numerical_stability`中讨论了良好初始化的必要性。\n",
    "深度学习框架提供默认随机初始化，\n",
    "也允许我们创建自定义初始化方法，\n",
    "满足我们通过其他规则实现初始化权重。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4ef73",
   "metadata": {
    "origin_pos": 44,
    "tab": [
     "paddle"
    ]
   },
   "source": [
    "默认情况下，PaddlePaddle会使用Xavier初始化权重矩阵，\n",
    "偏置参数设置为0。\n",
    "PaddlePaddle的`nn.initializer`模块提供了多种预置初始化方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15298a71",
   "metadata": {
    "origin_pos": 45
   },
   "source": [
    "### [**内置初始化**]\n",
    "\n",
    "让我们首先调用内置的初始化器。\n",
    "下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，\n",
    "且将偏置参数设置为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa9e54c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.967652Z",
     "iopub.status.busy": "2023-08-14T21:09:51.967115Z",
     "iopub.status.idle": "2023-08-14T21:09:51.974403Z",
     "shell.execute_reply": "2023-08-14T21:09:51.973607Z"
    },
    "origin_pos": 49,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[8], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
       "        [-0.14900780,  0.02392501, -0.10461706, -0.68165976,  0.07206899,\n",
       "          0.33707291, -0.07452518, -0.02119660]),\n",
       " Parameter containing:\n",
       " Tensor(shape=[8], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        paddle.nn.initializer.Normal(mean=0.0, std=0.01)\n",
    "        paddle.zeros(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight[0],net[0].state_dict()['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0def025",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb17f885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.977731Z",
     "iopub.status.busy": "2023-08-14T21:09:51.977200Z",
     "iopub.status.idle": "2023-08-14T21:09:51.984215Z",
     "shell.execute_reply": "2023-08-14T21:09:51.983427Z"
    },
    "origin_pos": 54,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(shape=[8], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
       "        [-0.14900780,  0.02392501, -0.10461706, -0.68165976,  0.07206899,\n",
       "          0.33707291, -0.07452518, -0.02119660]),\n",
       " Parameter containing:\n",
       " Tensor(shape=[8], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        paddle.nn.initializer.Constant(value = 1)\n",
    "        paddle.zeros(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight[0],net[0].state_dict()['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c067692",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。\n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，\n",
    "然后将第三个神经网络层初始化为常量值42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45c23c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.987555Z",
     "iopub.status.busy": "2023-08-14T21:09:51.986994Z",
     "iopub.status.idle": "2023-08-14T21:09:51.993666Z",
     "shell.execute_reply": "2023-08-14T21:09:51.992798Z"
    },
    "origin_pos": 59,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[8], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
      "       [-0.14900780,  0.02392501, -0.10461706, -0.68165976,  0.07206899,\n",
      "         0.33707291, -0.07452518, -0.02119660])\n",
      "Parameter containing:\n",
      "Tensor(shape=[8, 1], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
      "       [[-0.60320699],\n",
      "        [ 0.22128904],\n",
      "        [ 0.68972981],\n",
      "        [ 0.54654515],\n",
      "        [-0.52730417],\n",
      "        [ 0.00482953],\n",
      "        [-0.00573695],\n",
      "        [ 0.04480475]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        paddle.nn.initializer.XavierUniform(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        paddle.nn.initializer.Constant(42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight[0])\n",
    "print(net[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0151232",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "### [**自定义初始化**]\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506698b",
   "metadata": {
    "origin_pos": 64,
    "tab": [
     "paddle"
    ]
   },
   "source": [
    "同样，我们实现了一个`my_init`函数来应用到`net`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fced20c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:51.997043Z",
     "iopub.status.busy": "2023-08-14T21:09:51.996452Z",
     "iopub.status.idle": "2023-08-14T21:09:52.006404Z",
     "shell.execute_reply": "2023-08-14T21:09:52.005603Z"
    },
    "origin_pos": 68,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight [4, 8]\n",
      "Init weight [8, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 8], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
       "       [[-0.14900780,  0.02392501, -0.10461706, -0.68165976,  0.07206899,\n",
       "          0.33707291, -0.07452518, -0.02119660],\n",
       "        [-0.07113314,  0.45503086,  0.22228140, -0.32081056, -0.49508154,\n",
       "         -0.52045774,  0.65680486,  0.33165425]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        paddle.nn.initializer.XavierUniform(m.weight, -10, 10)\n",
    "        h = paddle.abs(m.weight) >= 5\n",
    "        h = paddle.to_tensor(h)\n",
    "        m = paddle.to_tensor(m.weight)\n",
    "        m *= h\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6571e",
   "metadata": {
    "origin_pos": 69
   },
   "source": [
    "注意，我们始终可以直接设置参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c64e96e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:52.009909Z",
     "iopub.status.busy": "2023-08-14T21:09:52.009230Z",
     "iopub.status.idle": "2023-08-14T21:09:52.015987Z",
     "shell.execute_reply": "2023-08-14T21:09:52.015181Z"
    },
    "origin_pos": 73,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[8], dtype=float32, place=Place(cpu), stop_gradient=False,\n",
       "       [42.       , 1.02392507, 0.89538294, 0.31834024, 1.07206893, 1.33707285,\n",
       "        0.92547482, 0.97880340])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.set_value(net[0].weight.numpy() + 1)\n",
    "val = net[0].weight.numpy()\n",
    "val[0, 0] = 42\n",
    "net[0].weight.set_value(val)\n",
    "net[0].weight[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0f2c9",
   "metadata": {
    "origin_pos": 75
   },
   "source": [
    "## [**参数绑定**]\n",
    "\n",
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8737fd2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-14T21:09:52.019441Z",
     "iopub.status.busy": "2023-08-14T21:09:52.018765Z",
     "iopub.status.idle": "2023-08-14T21:09:52.027246Z",
     "shell.execute_reply": "2023-08-14T21:09:52.026436Z"
    },
    "origin_pos": 79,
    "tab": [
     "paddle"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[8], dtype=bool, place=Place(cpu), stop_gradient=False,\n",
      "       [True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数。\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight[0] == net[4].weight[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c457aa5",
   "metadata": {
    "origin_pos": 82
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用 :numref:`sec_model_construction` 中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc2231",
   "metadata": {
    "origin_pos": 86,
    "tab": [
     "paddle"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/11778)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}